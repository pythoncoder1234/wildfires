{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Wildfires\n",
    "\n",
    "This repository is a mess, so when in doubt, run code from the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Format data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "def process(row):\n",
    "    return np.pad(np.nan_to_num(row), (3080 - len(row), 0))\n",
    "\n",
    "merge = pd.read_csv(\"merge.csv\")\n",
    "merge = merge.sort_values(['longitude', 'latitude'])\n",
    "merge['datetime'] = pd.to_datetime(merge['datetime'])\n",
    "merge = merge.loc[merge['datetime'].apply(lambda value: value.minute % 10 == 0)]\n",
    "grouped = merge.groupby('datetime')\n",
    "\n",
    "wind = grouped['speed'].apply(process).to_list()\n",
    "fire = grouped['Power'].apply(process).to_list()\n",
    "wind = normalize(wind) * 10\n",
    "fire = normalize(fire) * 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:18:39.561099Z",
     "start_time": "2023-11-04T16:18:33.188506Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define and train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_count):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        feature_cnt = np.linspace(input_size, output_size, hidden_count, dtype=int)\n",
    "        self.layers = nn.ModuleList([nn.Linear(feature_cnt[i], feature_cnt[i + 1]) for i in range(len(feature_cnt) - 1)])\n",
    "        print(self.layers)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        self.labels = torch.from_numpy(labels).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:18:40.525749Z",
     "start_time": "2023-11-04T16:18:39.562268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        test_loss.append(test(model, criterion, test_loader))\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss.append(total_loss)\n",
    "\n",
    "        # Print the average loss for this epoch\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    return train_loss, test_loss\n",
    "\n",
    "\n",
    "def test(model: MLP, criterion, dl):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(dl):\n",
    "        predicted = model(inputs)\n",
    "        loss = criterion(predicted, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:18:40.529983Z",
     "start_time": "2023-11-04T16:18:40.527708Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1313, 6160)\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=6160, out_features=5390, bias=True)\n",
      "  (1): Linear(in_features=5390, out_features=4620, bias=True)\n",
      "  (2): Linear(in_features=4620, out_features=3850, bias=True)\n",
      "  (3): Linear(in_features=3850, out_features=3080, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=6160, out_features=5390, bias=True)\n",
      "  (1): Linear(in_features=5390, out_features=4620, bias=True)\n",
      "  (2): Linear(in_features=4620, out_features=3850, bias=True)\n",
      "  (3): Linear(in_features=3850, out_features=3080, bias=True)\n",
      ")\n",
      "Epoch [1/10], Loss: 0.027945497929297312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m mlp \u001B[38;5;241m=\u001B[39m MLP(data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], fire\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(mlp\u001B[38;5;241m.\u001B[39mlayers)\n\u001B[0;32m---> 17\u001B[0m train_loss, test_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSGD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_loss\u001B[39m(train_loss, test_loss):\n\u001B[1;32m     21\u001B[0m     plt\u001B[38;5;241m.\u001B[39mplot(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(train_loss) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), train_loss, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 13\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[1;32m     11\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m---> 13\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     15\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectNebulus/venv/lib/python3.10/site-packages/torch/_tensor.py:503\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    495\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    496\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    501\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    502\u001B[0m     )\n\u001B[0;32m--> 503\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectNebulus/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:254\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    249\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    251\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 254\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = np.hstack((fire, wind))\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(data[:-1], fire[1:])\n",
    "train_dl = DataLoader(CustomDataset(train_x, train_y), batch_size=10, shuffle=True)\n",
    "test_dl = DataLoader(CustomDataset(test_x, test_y), batch_size=10, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "loss_func = nn.MSELoss()\n",
    "mlp = MLP(data.shape[1], fire.shape[1], 5)\n",
    "print(mlp.layers)\n",
    "train_loss, test_loss = train(mlp, train_dl, test_dl, loss_func, optim.SGD(mlp.parameters(), lr=0.01), epochs)\n",
    "\n",
    "\n",
    "def plot_loss(train_loss, test_loss):\n",
    "    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training loss\")\n",
    "    plt.plot(range(1, len(test_loss) + 1), test_loss, label=\"Test loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss ($L2$)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss(train_loss, test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:18:54.758217Z",
     "start_time": "2023-11-04T16:18:40.532171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "arr = fire[1300]\n",
    "arr[arr == 0] = np.nan\n",
    "axis = plt.matshow(arr.reshape(55, 56), fignum=fig, cmap=\"autumn_r\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T17:43:10.270172Z",
     "start_time": "2023-11-04T17:43:09.044809Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
