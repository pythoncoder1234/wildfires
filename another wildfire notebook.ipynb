{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfires\n",
    "\n",
    "Create directories (if they don't exist already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "for dir_name in [\"wind_interp\", \"fire_grid, lag\", \"correlation\"]:\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fire and wind data saved in csv file. The fire data is every 10 mins.\n",
    "The wind data is every 60 mins, and interpolated to 10 mins.\n",
    "\n",
    "Save the generated fire data in 2D matrices under fire_grid directory\n",
    "Save the generated wind data in 2D matrices under wind_interp directory."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRE_DF_PATH = \"fire_df_08_05.csv\"\n",
    "WIND_DF_PATH = \"wind_df_08_05.csv\"\n",
    "\n",
    "from timeinterp import *\n",
    "time_interp(data_path=WIND_DF_PATH)\n",
    "\n",
    "from preprocessing import *\n",
    "preprocess(data_path=FIRE_DF_PATH)\n",
    "\n",
    "from cross_correlation import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fire = pd.read_csv(\"fire_df_08_01.csv\")\n",
    "wind = pd.read_csv(\"wind_df_08_01.csv\")\n",
    "\n",
    "for df in [fire, wind]:\n",
    "    df['datetime'] = df['datetime'].apply(lambda time: pd.to_datetime(time).replace(second=0, microsecond=0))\n",
    "    df[['longitude', 'latitude']] = df[['longitude', 'latitude']].round(5)\n",
    "\n",
    "merge = pd.merge(fire, wind, how=\"outer\", on=[\"datetime\", \"longitude\", \"latitude\"])\n",
    "merge.to_csv(\"merge.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fire = pd.read_csv(\"fire_df_08_01.csv\")\n",
    "wind = pd.read_csv(\"wind_df_08_01.csv\")\n",
    "\n",
    "for df in [fire, wind]:\n",
    "    df['datetime'] = df['datetime'].apply(lambda time: pd.to_datetime(time).replace(second=0, microsecond=0))\n",
    "    df[['longitude', 'latitude']] = df[['longitude', 'latitude']].round(5)\n",
    "\n",
    "merge = pd.merge_ordered(fire, wind, how=\"left\", on=['datetime', 'longitude', 'latitude'], fill_method='ffill')\n",
    "merge.to_csv(\"merge.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_fire(row):\n",
    "    return np.nan_to_num(np.array(row))\n",
    "\n",
    "merge = pd.read_csv(\"merge.csv\")\n",
    "merge = merge.sort_values(['longitude', 'latitude'])\n",
    "grouped = merge.groupby('datetime')\n",
    "\n",
    "wind = grouped['speed'].apply(np.array).to_list()\n",
    "fire = grouped['Power'].apply(process_fire).to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:27:31.096581Z",
     "start_time": "2023-10-07T22:27:30.873783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        self.labels = torch.from_numpy(labels).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate total loss\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:56:36.932559Z",
     "start_time": "2023-10-07T22:56:36.928684Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 6160)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m ds \u001B[38;5;241m=\u001B[39m CustomDataset(data[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], data[\u001B[38;5;241m1\u001B[39m:])\n\u001B[1;32m      7\u001B[0m dl \u001B[38;5;241m=\u001B[39m DataLoader(ds, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 8\u001B[0m mlp \u001B[38;5;241m=\u001B[39m \u001B[43mMLP\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m train(mlp, dl, nn\u001B[38;5;241m.\u001B[39mMSELoss(), optim\u001B[38;5;241m.\u001B[39mSGD(mlp\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m), \u001B[38;5;241m5\u001B[39m)\n",
      "Cell \u001B[0;32mIn[9], line 11\u001B[0m, in \u001B[0;36mMLP.__init__\u001B[0;34m(self, input_size, hidden_size, output_size)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(input_size, hidden_size)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mReLU()\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2 \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectNebulus/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[0;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_features \u001B[38;5;241m=\u001B[39m in_features\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_features \u001B[38;5;241m=\u001B[39m out_features\n\u001B[0;32m---> 96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m Parameter(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_features\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfactory_kwargs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bias:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m=\u001B[39m Parameter(torch\u001B[38;5;241m.\u001B[39mempty(out_features, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfactory_kwargs))\n",
      "\u001B[0;31mTypeError\u001B[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = np.hstack((fire, wind))\n",
    "print(data.shape)\n",
    "\n",
    "ds = CustomDataset(data[:-1], data[1:])\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "mlp = MLP(data.shape[1], 5000, data.shape[1] / 2)\n",
    "train(mlp, dl, nn.MSELoss(), optim.SGD(mlp.parameters(), lr=0.01), 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T22:55:28.789902Z",
     "start_time": "2023-10-07T22:55:28.690185Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
